Tennis Ball Speed Estimation using YOLOv8 and SORT TrackingThis repository contains the code and report for a computer vision mini-project focused on detecting, tracking, and estimating the speed of a tennis ball from video footage. This project implements a complete pipeline from video input to data analysis and visualization.Table of ContentsProject ObjectiveProblem StatementOur Approach & PipelineTools and TechnologiesModel & Tracker Selection RationaleResultsChallenges and LimitationsFuture ImprovementsProject StructureHow to RunProject ObjectiveThe primary goal of this project is to develop a robust computer vision pipeline capable of automatically analyzing sports video footage to extract key performance metrics. Specifically, the objective is to accurately estimate the instantaneous and average speed of a moving tennis ball, simulating a real-world sports-tech application used for coaching, broadcasting, and enhancing fan engagement.Problem StatementThe task is to design and implement a system that can take a video clip of a tennis match as input and perform the following actions:Detect the tennis ball in each frame of the video.Track the ball's movement across consecutive frames to form a continuous trajectory.Calculate the approximate speed of the ball in km/h by converting its pixel displacement to real-world distance.Visualize the results by overlaying the trajectory on the output video and plotting a speed-time graph.Our Approach & PipelineTo solve this problem, I developed a multi-stage pipeline that processes the video sequentially to produce the final analytics. The workflow is as follows:Video Input & Pre-processing: The pipeline begins by loading the input video. Each frame is then pre-processed to enhance detection accuracy. This includes applying CLAHE (Contrast Limited Adaptive Histogram Equalization) to improve the ball's visibility against the court and background, especially in varying lighting conditions.Object Detection: A custom-trained YOLOv8n model is used to detect the tennis ball in every frame. YOLOv8n was chosen for its excellent balance of speed and accuracy, making it suitable for real-time applications. The model outputs bounding box coordinates for any detected ball.Object Tracking: The detection data is fed into the SORT (Simple Online and Realtime Tracking) algorithm. SORT is responsible for assigning a consistent ID to the detected ball across frames, even if the detector momentarily fails. This allows us to build a continuous and reliable trajectory of the ball's movement.Data Filtering and Smoothing:The raw coordinates from the tracker are first filtered to remove noise and outliers.The trajectory data is then smoothed using interpolation and a Savitzky-Golay filter. This step is crucial for calculating a stable and realistic speed, as it minimizes the impact of minor detection jitters.Speed Calculation:A pixel-to-meter conversion ratio is established based on the known width of a tennis court (10.97 meters).The smoothed pixel coordinates are converted into real-world coordinates (in meters).The distance the ball travels between consecutive frames is calculated, and by using the video's frame rate (FPS), the instantaneous speed is computed (Speed = Distance / Time).Output Generation: The pipeline produces four key outputs:An annotated video showing the detected ball, its bounding box, and a fading trace of its trajectory.A CSV file containing the frame-by-frame coordinates of the ball.A plot image (.png) visualizing the ball's trajectory and a graph of its speed over time.Terminal output summarizing the key statistics, including average and peak speed.Here is a conceptual diagram of the pipeline:<!-- Add a conceptual diagram of the pipeline here. Example: -->Tools and TechnologiesThis project leverages a hybrid environment and a combination of powerful open-source libraries:Model Training:Google Colab: Used for training the YOLOv8 model on a custom dataset. The cloud GPU resources available on Colab significantly accelerated the training process.Pipeline Execution & Development:Visual Studio Code: Served as the primary IDE for writing, debugging, and running the main processing script (track3.py).Core Libraries:Python 3.x: The primary programming language for the project.OpenCV: Used for all video and image processing tasks, such as reading frames, applying CLAHE, and writing the output video.Ultralytics YOLOv8: The core object detection framework. I used a fine-tuned YOLOv8n model.Supervision: A high-level library used for annotating frames with bounding boxes, labels, and traces, simplifying the visualization code.SORT (from filterpy): The tracking algorithm used to maintain object identity across frames.NumPy & Pandas: Used for efficient numerical operations and data manipulation, especially for handling the coordinate data.Matplotlib & Scipy: Used for plotting the final graphs and for signal processing tasks like trajectory smoothing.Model & Tracker Selection RationaleWhy YOLOv8?The YOLO (You Only Look Once) family of models is renowned for its high speed and accuracy in object detection. I chose YOLOv8n (the nano version) specifically because it provides a fantastic trade-off between performance and computational cost. For a small, fast-moving object like a tennis ball, having a real-time detector is essential. While larger models like YOLOv8m or YOLOv8l might offer slightly better accuracy, YOLOv8n is lightweight enough to run smoothly without requiring a high-end GPU for inference, making the pipeline more accessible.Why SORT Tracker?For this specific problem, the goal is to track a single, primary object (the ball). The SORT algorithm is an excellent choice because it is computationally efficient and highly effective for this scenario. It primarily uses a Kalman filter to predict the object's location in the next frame and associates detections based on IoU (Intersection over Union). Unlike more complex trackers like DeepSORT, it does not use a deep learning model for re-identification, which makes it much faster and simpler to implement—a perfect fit for a project where speed and a clean implementation are key.ResultsThe pipeline was successfully implemented and tested on sample video footage. The system was able to detect and track the tennis ball, calculate its speed, and generate all the specified outputs.Final Statistics from a Sample Run:Video processing finished. Output video saved to 'outputs\ball_tracking_output_20250908_063149.mp4'
Coordinate data saved to 'outputs\ball_coordinates_20250908_063149.csv'
Ball actually detected in 198 frames. Enhanced trace provides better continuity.
Average Speed: 18.24 km/h | Peak Speed: 190.84 km/h
Plot saved to 'outputs\plots_20250908_063149.png'
Screenshot of Final Annotated Video:<!-- Add a screenshot of the final annotated video here. Example: -->Screenshot of Output Graph:<!-- Add a screenshot of the output graph here. Example: -->Challenges and LimitationsWhile the pipeline is functional, it is not perfect. Several challenges and limitations were identified during development:Detection Accuracy: The YOLOv8n model, while fast, can sometimes fail to detect the ball, especially during frames with extreme motion blur or when the ball is very small.Occlusion: The tracker may lose the ball if it is occluded by a player or the net for an extended period. The current implementation can handle short gaps through interpolation but will fail with longer occlusions.Camera Angle and Lens Distortion: The speed calculation assumes a fixed, 2D plane and does not account for camera lens distortion or the 3D movement of the ball (e.g., changes in depth). This means the calculated speed is an approximation.Single Camera View: Without multiple camera angles, it's impossible to get a true 3D trajectory, which limits the accuracy of the speed estimation.Future ImprovementsThis project serves as a strong foundation, and several enhancements could be made to improve its accuracy and capabilities:Use a Larger YOLO Model: To improve detection reliability, the pipeline could be upgraded to use a larger, more accurate model like YOLOv8m or YOLOv8l. While this would increase computational requirements, it would likely reduce the number of missed detections.Improve the Dataset: Training the model on a more diverse and larger dataset, including various lighting conditions, camera angles, and ball speeds, would significantly enhance its robustness.Advanced Tracking Algorithm: Implementing a more sophisticated tracker like DeepSORT or ByteTrack could improve tracking through short occlusions.Camera Calibration: To get a more accurate speed estimation, the camera could be calibrated to account for lens distortion and perspective. This would allow for a more precise mapping from pixel coordinates to real-world coordinates.3D Trajectory Reconstruction: With footage from two or more calibrated cameras, it would be possible to reconstruct the ball's 3D trajectory, leading to a much more accurate speed and position analysis.Project Structure.
├── trained-yolo/
│   └── t2-biggerDS+augmentation/
│       └── best.pt           # Trained YOLOv8 model weights
├── input/
│   └── tclip2.mp4            # Input video file
├── outputs/                  # Directory for all generated files
│   ├── ball_tracking_output_...mp4
│   ├── ball_coordinates_...csv
│   └── plots_...png
├── sort/                     # Source code for the SORT tracker
│   └── sort.py
└── track3.py                 # Main Python script to run the pipeline
How to RunClone the repository:git clone <your-repo-link>
cd <your-repo-directory>
Install dependencies:Make sure you have Python 3.8+ installed. Then, install the required packages.pip install -r requirements.txt
(Note: You may need to create a requirements.txt file based on the imports in track3.py)Place your files:Add your trained model weights (best.pt) to the trained-yolo/ directory.Place the input video you want to process in the input/ directory.Run the script:Execute the main tracking script from the terminal.python track3.py
Check the output:All output files (video, CSV, plot) will be saved in the outputs/ directory with a timestamp.
